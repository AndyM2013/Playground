{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Robitc Arm to Reach a Ball\n",
    "\n",
    "## Compare Learning Efficiency over DDPG, D4PG and A2C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "- [Hungryof, 模型中buffer的使用 (2018), CSDN](https://blog.csdn.net/Hungryof/article/details/82017595)\n",
    "- [M. Zhou, Simple implementation of Reinforcement Learning (A3C) using Pytorch (2018), Github](https://github.com/MorvanZhou/pytorch-A3C/blob/master/continuous_A3C.py)\n",
    "- [S. Zhang, Modularized Implementation of Deep RL Algorithms in PyTorch (2018), Github](https://github.com/ShangtongZhang/DeepRL/blob/master/deep_rl/network/network_heads.py)\n",
    "- [M. Lapan, Hands-on Deep Reinforcement Learning (2018), Github](https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/tree/master/Chapter14)\n",
    "- [Udacity, Deep Reinforcement Learning (2018), Github](https://github.com/udacity/deep-reinforcement-learning/tree/master/ddpg-pendulum)\n",
    "- [P. Emami, Deep Deterministic Policy Gradients in TensorFlow (2016), Github](https://pemami4911.github.io/blog/2016/08/21/ddpg-rl.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstrct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Incentive\n",
    "\n",
    "It's always facinating to see how we can leverge the power of reinforcement learning and train a robot to learn interacting with the environemnt via self trials and errors. In this task, I will experiment and compare the efficiency among different popular algorithms in reinforcement learning. This would not only help us grasp a general view of these algorithms but also furhter convince us of how the potentials of reinforcement learning to be when dealing with more complicated tasks.\n",
    "\n",
    "\n",
    "## Training Environment\n",
    "\n",
    "In the domain of reinforcement learning, the components consist of an environment and an agent/agents and the information exchanged between the two. First, the environment sends info of current situation to the agent, then agent reponds back in the best possible way, and then then environment sends back the reward and the info of next situation to the agent again, and so on. Here, I will use the environment [**Reacher**](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md#reacher), which is developed by Unity and modified by Udacity to impplement the experiment. It saves us the trouble of building up the environment and configuring the reward mechanism. That enables us to just focus on implementing, adjusting and comparing the RL algorithms as a beginner in this domain.\n",
    "\n",
    "In `Reacher`, the robotic arm needs to learn how to control and move a ball around. The longer time it sticks to the ball and control it, the more rewards it accumulates. The observation states of environment consist of 33 variables, and all are in continuous space.\n",
    "\n",
    "Respecting to the robotic arm, there are two scenarios, one being single robotic agent, and the other being multiple robotic agents, which are 20 agents in total, each with their own copy of environment. As we now have single and multiple agents scenarios, we can then explore and compare the learning efficiency between the two. Each agent has 4 action variables, all in continuous space within -1 and 1. Both single and multiple agents environments can be downloaded in [Udacity Github](https://github.com/udacity/deep-reinforcement-learning/tree/master/p2_continuous-control).\n",
    "\n",
    "\n",
    "## Models/Algorithms Experimented\n",
    "\n",
    "In single angent case, [Deep Deterministic Policy Gradient(DDPG)](https://arxiv.org/abs/1509.02971) and [Distributed Distributional Deterministic Policy Gradient(D4PG)](https://arxiv.org/abs/1804.08617) are used. We know that when training based on single agent, the sequence of trasition states/experiences will be correlated, so that off-policy such as DDPG/D4PG will be more suitable in this case. In these models, the practice of sampling on **replay memory** breaks up the correlation between transitions.\n",
    "\n",
    "In multiple agents case, [Synchronous Advantage Actor Critic (A2C)](https://blog.openai.com/baselines-acktr-a2c/) is used. A2C is a synchronous version of [A3C](https://arxiv.org/abs/1602.01783). A2C is easier to be implemented and in some studies, its performance is better than A3C as well. In the case of training on multiple agents, for each own has unique transition experience, the collective transitions will resolve the problem of sequential correlation by nature. Furthermore, we can even enjoy the training stability brough on by on-policy learning algorithms.\n",
    "\n",
    "\n",
    "## Quick Summary\n",
    "\n",
    "In this task environment, if the agent's arm is in the goal location, then a reward of +0.1 is given. The longer time the agent is able to maintain its arm in the goal location, the more rewards it accumulates.\n",
    "\n",
    "From the results shown later on, we can clearly tell that **A2C** significantly outperforms than the rest two algorithms in terms of training speed, which is not surprising, nonetheless the improvement is really impressive. In this case specifically, the A2C successfully trains the agents and enbale them to accumulate rewards above 30 (the target total rewards) in less than **100 training episodes**. While D4PG requires more than **3000 episodes** and DDPG **isn't even able to sovle the task** no matter how much time it trains the agent.\n",
    "\n",
    "## Structure of The Report\n",
    "\n",
    "**<mark>1. How The Agent Interact with The Environment</mark>**\n",
    "\n",
    "In this section, I will display codes of how the agent interacts with the environment (assuming it's single agent scenario), sending its action values to the environment and reveiving feedbacks of rewards and next observation state values.\n",
    "\n",
    "**<mark>2. Train on Single Agent Scenario</mark>**\n",
    "\n",
    "In single agent environment, I will experiment on two algorithms - DDPG and D4PG. This section will cove how the model is built up step by setp, including its actor and critic network structure, the setting for replay memory, loss function etc. \n",
    "\n",
    "**<mark>3. Train on Multiple Agents Scenario</mark>**\n",
    "\n",
    "I experiment A2C model on multi-agent environment, the code for A2C is rather different from the codes for DDPG and D4PG. It uses multi-agents to collect experience and update network parameters. This section will cover how A2C is built up and experience collection and learning process.\n",
    "\n",
    "**<mark>4. Comparison of All Models</mark>**\n",
    "\n",
    "The rolling score of each algorithm will be presented at the same plot, so that we can compare how they perform over training. \n",
    "\n",
    "**<mark>5. Further Improvement</mark>**\n",
    "\n",
    "Some general ideas of how I should better generalize the code and improve the setting of model hyper-parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How Agent Interact with The Environment\n",
    "\n",
    "Here I am going to give a brief introduction of how Unity environment looks like. In addition, how the agent can interact with the environment in codes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background of The Environment\n",
    "\n",
    "Each environment contains multiple brains. Brains control how the environment responds based on the action inputs it receives. Here in this task, the default brain(the first one) is used. \n",
    "\n",
    "`env` object contains background information for each brain. It includes \n",
    "- `agents`: number of agents\n",
    "- `vector_observations`: dimension of (number of agents, number of observation states).\n",
    "\n",
    "`brain` object contains,\n",
    "- `vector_action_space_size`: the number of actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "def printmd(string):\n",
    "    display(Markdown(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_path = '/Users/tomlin/Documents/Github-Repository/RLND/RLND-dataset/p2-continuous-control/Reacher_single.app'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "\n",
    "# Import the environment.\n",
    "env = UnityEnvironment(file_name=env_path)\n",
    "\n",
    "# Get default brain name.\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# Reset the environment -> switch to training(episodical) mode,\n",
    "# and extract environment information out by indexing on brain_name.\n",
    "env_info = env.reset(train_mode=True)[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Number of agents:** 1"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "**Size of each action:** 4"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "**There are** 1 **agents. Each observes a state with length:** 33"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "**The state for the agent looks like:**\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.00000000e+00, -4.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00, -4.37113883e-08,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00, -1.00000000e+01,  0.00000000e+00,\n",
       "        1.00000000e+00, -0.00000000e+00, -0.00000000e+00, -4.37113883e-08,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  5.75471878e+00, -1.00000000e+00,\n",
       "        5.55726671e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
       "       -1.68164849e-01])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "printmd('**Number of agents:** {}'.format(num_agents))\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "printmd('\\n**Size of each action:** {}'.format(action_size))\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "printmd('\\n**There are** {} **agents. Each observes a state with length:** {}'.format(states.shape[0], state_size))\n",
    "printmd('\\n**The state for the agent looks like:**\\n')\n",
    "display(states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Interact with Environment\n",
    "\n",
    "For every new episode, the environment needs to be `reset`. Then the agent responds by `self-defined method(act)` to the states it receives from the environment. The environment uses `step` to take in current actions from the agent and update itself with *new states*, *rewards* and  *dones(mark of end of episode)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_info = env.reset(train_model=True)[brain_name] # reset the environment to the default brain\n",
    "states = env_info.vector_observations # the initial states\n",
    "scores = np.zero(num_agents) # rewards accumulator\n",
    "\n",
    "while True:\n",
    "    actions = agent.act(states) # agent acts according to the states\n",
    "    env_info = env.step(actions)[brain_name] # send actions to env and extract the updated env info of the default brain\n",
    "    next_states = env_info.vector_observations\n",
    "    states = next_states\n",
    "    rewards = env_info.rewards\n",
    "    scores += rewards\n",
    "    dones = env_info.local_done # see if episode ends\n",
    "    if np.any(dones):\n",
    "        break    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on Single Angent Scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDPG\n",
    "\n",
    "First, I import some self-defined modules to configure the whole setting before training. The modules includes,\n",
    "\n",
    "1. `ddpg_model`: Module file containing classes of Actor and Critic neural network structure for DDPG.\n",
    "2. `noise`: Ornstein-Uhlenbeck Noise process for exploration purpose in DDPG agent.\n",
    "3. `replay_memory`: Collect and uniformly random sample transition experience for training.\n",
    "4. `ddpg_agent`: Module file defining how an DDPG agent interact with the environment and implement training process.\n",
    "\n",
    "Following will be a series of code snippets to highligh the most importance part in each self-defined module. It helps us get to the core in each module more quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Structure\n",
    "\n",
    "Most of the network's structure follows what being implemented in original [DDGP paper](https://arxiv.org/abs/1509.02971), and the major chunk of the script is referred from [Udacity's DDPG template](https://github.com/udacity/deep-reinforcement-learning/tree/master/ddpg-pendulum). The critic network takes in the action in last hidden layer for computing Q value. \n",
    "\n",
    "However, for training efficiency, some hyperparameters are adjusted in this case. For instance, both actor and critic network contain two hidden layers (each with size of **128** and **64** units). Code below is the initialization of Critic Network. Actor Netork shares similar structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excerpt of initialization on critic network with two layers of size 128 and 64.\n",
    "class Critic(nn.Module):\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=128, fc2_units=64):\n",
    "        super(Critic, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units+action_size, fc2_units) # actions are included in latter phase\n",
    "        self.fc3 = nn.Linear(fc2_units, 1)\n",
    "        self.reset_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight Initialization\n",
    "\n",
    "In initialization, the weights of both critic and actor networks adopt **Xaiver initialization**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hidden_init(layer):\n",
    "    '''\n",
    "    Provide fan in (the number of input units) of each hidden layer\n",
    "    as the component of normalizer.\n",
    "    :param\n",
    "        layer: hidden layer from PyTorch nn.Module object\n",
    "    :return\n",
    "        (-lim, lim): tuple of min and max value for uniform distribution\n",
    "    '''\n",
    "\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay Memory\n",
    "\n",
    "Here, the size of replay memory is set only **100,000** in order to lessen the computing pressure. The replay memory samples in **uniform random sampling**, and is sent to *device(cuda or cpu)* to train the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excerpt of replay memory object.\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # find out model being kept in cpu or cuda\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(\n",
    "            device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(\n",
    "            device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action Exploration\n",
    "\n",
    "The Ornstein-Uhlenbeck noise is added in action for action exploration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excerpt of DDPG agent object.\n",
    "\n",
    "class Agent():\n",
    "    '''Interact with and learn from environment.'''\n",
    "\n",
    "    def act(self, state, mode):\n",
    "        '''Returns actions for given state as per current policy.\n",
    "        Params\n",
    "        ======\n",
    "            state (array): current state\n",
    "            mode (string): train or test\n",
    "            epsilon (float): for epsilon-greedy action selection\n",
    "        '''\n",
    "        state = torch.from_numpy(state).unsqueeze(0).float().to(device) # shape of state (1, state_size)\n",
    "        self.actor_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action = self.actor_local(state).cpu().data.numpy()\n",
    "        self.actor_local.train()\n",
    "\n",
    "        if mode == 'test':\n",
    "            return np.clip(action, -1, 1)\n",
    "\n",
    "        elif mode == 'train': # if train, then add OUNoise in action\n",
    "            action += self.noise.sample()\n",
    "            return np.clip(action, -1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "\n",
    "The determinstic policy gradient is different from stochastic policy graident. It doesn't inlude the log probability of the action. \n",
    "\n",
    "That said, the policy(actor) graident is the gradient of the critic network w.r.t. the action, multiplied by the graident of the actor network w.r.t the network parameters. Read further detail in this [post](https://pemami4911.github.io/blog/2016/08/21/ddpg-rl.html). \n",
    "\n",
    "On the other hand, the critic value loss is just the regular TD error. Hence, the policy loss and critic value loss are defined as:\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "L_{policy} & = - \\frac{1}{M}\\sum_{i=1}^M Q(S_i, \\mu(S_i \\;\\vert\\; \\theta^\\mu) \\;\\big\\vert\\; \\theta^Q) \\\\\n",
    "L_{value} & = \\frac{1}{M}\\sum_{i=1}^M(R + \\gamma Q(S_{i+1}, \\mu^\\prime(S_{i+1} \\;\\vert\\; \\theta^{\\mu^\\prime}) \\;\\big\\vert\\; \\theta^{Q^\\prime})- Q(S_i, A_i \\;\\vert\\; \\theta^Q))^2\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excerpt of DDPG agent object.\n",
    "\n",
    "class Agent():\n",
    "    '''Interact with and learn from environment.'''\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "    \"\"\"Update policy and value parameters using given batch of experience tuples.\n",
    "    Q_targets = r + γ * critic_target(next_state, actor_target(next_state))\n",
    "    where:\n",
    "        actor_target(state) -> action\n",
    "        critic_target(state, action) -> Q-value\n",
    "    Params\n",
    "    ======\n",
    "        experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples\n",
    "        gamma (float): discount factor\n",
    "    \"\"\"\n",
    "    states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "    # ---------------------------- update critic ---------------------------- #\n",
    "    # Get predicted next-state actions and Q values from target models\n",
    "    actions_next = self.actor_target(next_states)\n",
    "    Q_targets_next = self.critic_target(next_states, actions_next)\n",
    "    # Compute Q targets for current states (y_i)\n",
    "    Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "    # Compute critic loss\n",
    "    Q_expected = self.critic_local(states, actions)\n",
    "    critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "\n",
    "    # Minimize the loss\n",
    "    self.critic_optimizer.zero_grad()\n",
    "    critic_loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(self.critic_local.parameters(), 1) # clip gradient to max 1\n",
    "    self.critic_optimizer.step()\n",
    "\n",
    "    # ---------------------------- update actor ---------------------------- #\n",
    "    # Compute actor loss\n",
    "    actions_pred = self.actor_local(states)\n",
    "    actor_loss = -self.critic_local(states, actions_pred).mean()\n",
    "\n",
    "    # Minimize the loss\n",
    "    self.actor_optimizer.zero_grad()\n",
    "    actor_loss.backward()\n",
    "    self.actor_optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight Update\n",
    "\n",
    "In training process, the weights in the models are soft-updated in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excerpt of DDPG agent object.\n",
    "\n",
    "class Agent():\n",
    "    '''Interact with and learn from environment.'''\n",
    "    \n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "        Params\n",
    "        ======\n",
    "            local_model: PyTorch model (weights will be copied from)\n",
    "            target_model: PyTorch model (weights will be copied to)\n",
    "            tau (float): interpolation parameter\n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameters in a Nutshell\n",
    "\n",
    "Followings are the overview of hyper-parameter setting,\n",
    "\n",
    "- Learning Rate (Actor/Critic): 1e-4\n",
    "- Weight Decay: 1e-2\n",
    "- Batch Size: 64\n",
    "- Buffer Size: 100000\n",
    "- Gamma: 0.99\n",
    "- Tau: 1e-3\n",
    "- Repeated Learning per time: 10\n",
    "- Learning Happened per timestep: 20\n",
    "- Max Gradient Clipped for Critic: 1\n",
    "- Hidden Layer 1 Size: 128\n",
    "- Hidden Layer 2 Size: 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Training Function\n",
    "\n",
    "Below, I define a training function `train_ddpg()` to monitor the training progress and save the model after training is finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import deque\n",
    "\n",
    "# train the agent\n",
    "def train_ddpg(agent, memory, n_episodes=10, mode='train', \n",
    "        actor_pth='./checkpoint/ddpg_actor_checkpoint.pth',\n",
    "        critic_pth='./checkpoint/ddpg_critic_checkpoint.pth'):\n",
    "    '''Set up training's configuration and print out episodic performance measures, such as avg scores, avg loss.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        agent (class object)\n",
    "        memory (class attribute): agent's attribute for memory size tracking\n",
    "        mode (string): 'train' or 'test', when in test mode, the agent acts in greedy policy only\n",
    "        pth (string path): file name for the checkpoint    \n",
    "    '''\n",
    "    \n",
    "    scores = []\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    c_loss_window = deque(maxlen=100)\n",
    "    a_loss_window = deque(maxlen=100)\n",
    "\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]  # reset the environment and activate train_mode\n",
    "        state = env_info.vector_observations[0]            # get the current state\n",
    "        score = 0\n",
    "        agent.running_c_loss = 0\n",
    "        agent.running_a_loss = 0\n",
    "        agent.training_cnt = 0\n",
    "        # agent.reset() # reset OUNoise\n",
    "        \n",
    "        while True:\n",
    "            action = agent.act(state, mode)\n",
    "            env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "            next_state = env_info.vector_observations[0]      # get the next state\n",
    "            reward = env_info.rewards[0]                   # get the reward\n",
    "            done = env_info.local_done[0]                  # see if episode has finished        \n",
    "\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "\n",
    "            score += reward\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        scores_window.append(score)\n",
    "        scores.append(score)\n",
    "        c_loss_window.append(agent.running_c_loss/(agent.training_cnt+0.0001)) # avoid zero\n",
    "        a_loss_window.append(agent.running_a_loss/(agent.training_cnt+0.0001)) # avoid zero\n",
    "        print('\\rEpisode {:>4}\\tAverage Score:{:>6.3f}\\tMemory Size:{:>5}\\tCLoss:{:>12.8f}\\tALoss:{:>10.6f}'.format(\n",
    "            i_episode, np.mean(scores_window), len(memory), np.mean(c_loss_window), np.mean(a_loss_window)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {:>4}\\tAverage Score:{:>6.3f}\\tMemory Size:{:>5}\\tCLoss:{:>12.8f}\\tALoss:{:>10.6f}'.format(\n",
    "                i_episode, np.mean(scores_window), len(memory), np.mean(c_loss_window), np.mean(a_loss_window)))\n",
    "        if np.mean(scores_window) >= 31:\n",
    "            break\n",
    "    torch.save(agent.actor_local.state_dict(), actor_pth)\n",
    "    torch.save(agent.critic_local.state_dict(), critic_pth)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Result - Failed\n",
    "\n",
    "In the experiment, I set up the training triggered at every **20 timesteps**. Weight-update will iterate for **10 times** in each training.\n",
    "\n",
    "In addition, the **critic gradient is clipped** with maximum value as 1 to enhance the training stability. As you will see below, the **episodic reward lingers around 0.04 to 0.05** in the first 1000 episodes, which pretty much means the agent not able to learn from these experiences at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pictor of training Result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D4PG\n",
    "\n",
    "As you will see later, the DDPG model doesn't solve the task successfully, so I turn to another algorithm - [D4PG](https://arxiv.org/abs/1804.08617), which is the most updated RL algorithm in 2018. The code script is mainly referred from this book - [Deep-Reinforcement-Learning-Hands-On](https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On).\n",
    "\n",
    "Again, I import some self-defined modules to configure the whole setting before training. The modules includes,\n",
    "\n",
    "1. `d4pg_model`: Module file containing classes of Actor and Critic neural network structure for D4PG.\n",
    "2. `replay_memory`: Collect and uniformly random sample transition experience for training.\n",
    "3. `d4pg_agent`: Module file defining how an D4PG agent interact with the environment and implement training process.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Structure\n",
    "\n",
    "I follow up the same model structure as specified in DDPG, except for critic network, the output needs to be changed to **N_ATOMS** output. Both Critic and Actor have two hidden layers with size 128, 64 each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class D4PGCritic(nn.Module):\n",
    "    def __init__(self, obs_size, act_size, seed, n_atoms, v_min, v_max):\n",
    "        super(D4PGCritic, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.obs_net = nn.Sequential(\n",
    "            nn.Linear(obs_size, 128),\n",
    "            nn.ReLU(),\n",
    "        ) # observation state\n",
    "\n",
    "        self.out_net = nn.Sequential(\n",
    "            nn.Linear(128 + act_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, n_atoms)\n",
    "        ) # concatenate state and action, and output N_ATOMS\n",
    "\n",
    "        delta = (v_max - v_min) / (n_atoms - 1) # v_max, v_min, n_atoms are given arguments\n",
    "        self.register_buffer(\"supports\", torch.arange(v_min, v_max + delta, delta))\n",
    "\n",
    "    def forward(self, x, a):\n",
    "        obs = self.obs_net(x)\n",
    "        return self.out_net(torch.cat([obs, a], dim=1)) # output N_ATOMS\n",
    "\n",
    "    def distr_to_q(self, distr):\n",
    "        \"\"\"Used when updating actor netork.\"\"\"\n",
    "        weights = F.softmax(distr, dim=1) * self.supports\n",
    "        res = weights.sum(dim=1)\n",
    "        return res.unsqueeze(dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay Memory\n",
    "\n",
    "In order to align to datatype required in D4PG agent, which is referred from [Deep-Reinforcement-Learning-Hands-On](https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On), the sampling is done via function `sample2()` defined in replay memory object. Replay memory is set size 100000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excerpt of replay memory object.\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "    \n",
    "    def sample2(self, device=device):\n",
    "    \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "    experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "    states, actions, rewards, next_states, dones = [], [], [], [], []\n",
    "\n",
    "    for exp in experiences:\n",
    "        states.append(exp.state.squeeze(0))\n",
    "        actions.append(exp.action.squeeze(0))\n",
    "        rewards.append(exp.reward)\n",
    "        dones.append(exp.done)\n",
    "        next_states.append(exp.next_state.squeeze(0))\n",
    "\n",
    "    states_v = torch.Tensor(np.array(states, dtype=np.float32)).to(device)\n",
    "    actions_v = torch.Tensor(np.array(actions, dtype=np.float32)).to(device)\n",
    "    rewards_v = torch.Tensor(np.array(rewards, dtype=np.float32)).to(device)\n",
    "    next_states_v = torch.Tensor(np.array(next_states, dtype=np.float32)).to(device)\n",
    "    dones_v = torch.ByteTensor(dones).to(device)\n",
    "\n",
    "    return states_v, actions_v, rewards_v, next_states_v, dones_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action Exploration\n",
    "\n",
    "One minor difference from DDPG is the action exploration. In D4PG it uses **simple random noise from normal distribution** instead of OU noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excerpt of D4PG agent object.\n",
    "\n",
    "class AgentD4PG():\n",
    "    \"\"\"\n",
    "    Agent implementing noisy agent\n",
    "    \"\"\"\n",
    "    \n",
    "    def act(self, states, mode):\n",
    "    states_v = torch.Tensor(np.array(states, dtype=np.float32)).to(self.device)\n",
    "\n",
    "    self.actor_local.eval()\n",
    "    with torch.no_grad():\n",
    "        mu_v = self.actor_local(states_v)\n",
    "        actions = mu_v.data.cpu().numpy()\n",
    "    self.actor_local.train()\n",
    "\n",
    "    if mode == \"test\":\n",
    "        return np.clip(actions, -1, 1)\n",
    "\n",
    "    elif mode == \"train\":\n",
    "        actions += self.epsilon * np.random.normal(size=actions.shape) # use simple normal random noise instead of OU noise\n",
    "        actions = np.clip(actions, -1, 1)\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "\n",
    "The D4PG can train on *multiple transition trajectory(N-Steps)*, but I choose to train on **one-step** for its simplicity. However, according to other reviews, one-step training is the most unstable one and not recoomended, but I still go for it anyway. The following loss and training is based on one-step transition trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excerpt of D4PG agent object.\n",
    "\n",
    "REWARD_STEPS = 1 # transition trajectory\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class AgentD4PG():\n",
    "    \"\"\"\n",
    "    Agent implementing noisy agent\n",
    "    \"\"\"\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update policy and value parameters using given batch of experience tuples.\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples\n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # ---------------------------- update critic ---------------------------- #\n",
    "        crt_distr_v = self.critic_local(states, actions) # N_ATOMS outputs on dimension 1\n",
    "        last_act_v = self.actor_target(next_states)\n",
    "        last_distr_v = F.softmax(self.critic_target(next_states, last_act_v), dim=1) # largest value along dimension 1\n",
    "\n",
    "        proj_distr_v = distr_projection(last_distr_v, rewards, dones,\n",
    "                                        gamma=gamma ** REWARD_STEPS, device=device) # self-defined function\n",
    "\n",
    "        prob_dist_v = -F.log_softmax(crt_distr_v, dim=1) * proj_distr_v\n",
    "        critic_loss_v = prob_dist_v.sum(dim=1).mean()\n",
    "\n",
    "        # Minimize the loss\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss_v.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.critic_local.parameters(), 1) # clip gradient to max 1\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # ---------------------------- update actor ---------------------------- #\n",
    "        # Compute actor loss\n",
    "        actions_pred = self.actor_local(states)\n",
    "        crt_distr_v  = self.critic_local(states, actions_pred) # N_ATOMS output\n",
    "        actor_loss_v = -self.critic_local.distr_to_q(crt_distr_v)\n",
    "        actor_loss_v = actor_loss_v.mean()\n",
    "\n",
    "        # Minimize the loss\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss_v.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        \n",
    "        \n",
    "         \n",
    "######### Self-defined funciton #########\n",
    "\n",
    "def distr_projection(next_distr_v, rewards_v, dones_mask_t, gamma, device=\"cpu\"):\n",
    "    \"\"\"Self-defined function being used in updating critic network.\"\"\"\n",
    "    next_distr = next_distr_v.data.cpu().numpy()\n",
    "    rewards = rewards_v.data.cpu().numpy()\n",
    "    dones_mask = dones_mask_t.cpu().numpy().astype(np.bool)\n",
    "    batch_size = len(rewards)\n",
    "    proj_distr = np.zeros((batch_size, N_ATOMS), dtype=np.float32)\n",
    "\n",
    "    for atom in range(N_ATOMS):\n",
    "        tz_j = np.minimum(Vmax, np.maximum(Vmin, rewards + (Vmin + atom * DELTA_Z) * gamma))\n",
    "        b_j = (tz_j - Vmin) / DELTA_Z\n",
    "        l = np.floor(b_j).astype(np.int64)\n",
    "        u = np.ceil(b_j).astype(np.int64)\n",
    "        eq_mask = u == l\n",
    "        proj_distr[eq_mask, l[eq_mask]] += next_distr[eq_mask, atom]\n",
    "        ne_mask = u != l\n",
    "        proj_distr[ne_mask, l[ne_mask]] += next_distr[ne_mask, atom] * (u - b_j)[ne_mask]\n",
    "        proj_distr[ne_mask, u[ne_mask]] += next_distr[ne_mask, atom] * (b_j - l)[ne_mask]\n",
    "\n",
    "    if dones_mask.any():\n",
    "        proj_distr[dones_mask] = 0.0\n",
    "        tz_j = np.minimum(Vmax, np.maximum(Vmin, rewards[dones_mask]))\n",
    "        b_j = (tz_j - Vmin) / DELTA_Z\n",
    "        l = np.floor(b_j).astype(np.int64)\n",
    "        u = np.ceil(b_j).astype(np.int64)\n",
    "        eq_mask = u == l\n",
    "        eq_dones = dones_mask.copy()\n",
    "        eq_dones[dones_mask] = eq_mask\n",
    "        if eq_dones.any():\n",
    "            proj_distr[eq_dones, l[eq_mask]] = 1.0\n",
    "        ne_mask = u != l\n",
    "        ne_dones = dones_mask.copy()\n",
    "        ne_dones[dones_mask] = ne_mask\n",
    "        if ne_dones.any():\n",
    "            proj_distr[ne_dones, l[ne_mask]] = (u - b_j)[ne_mask]\n",
    "            proj_distr[ne_dones, u[ne_mask]] = (b_j - l)[ne_mask]\n",
    "    return torch.FloatTensor(proj_distr).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight Update\n",
    "\n",
    "The weights are soft-updated by `soft_update()`, the same as in DDPG."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameter in a Nutshell\n",
    "\n",
    "Followings are the overview of hyper-parameter setting,\n",
    "\n",
    "- Learning Rate (Actor/Critic): 1e-4\n",
    "- Batch Size: 64\n",
    "- Buffer Size: 100000\n",
    "- Gamma: 0.99\n",
    "- Tau: 1e-3\n",
    "- Repeated Learning per time: 10\n",
    "- Learning Happened per timestep: 150\n",
    "- Max Gradient Clipped for Critic: 1\n",
    "- N-step: 1\n",
    "- N-Atoms: 51\n",
    "- Vmax: 10\n",
    "- Vmin: -10\n",
    "- Hidden Layer 1 Size: 128\n",
    "- Hidden Layer 2 Size: 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Training Function\n",
    "\n",
    "The training function `train_d4pg()` is pretty much the same as `train_ddpg()`, see full code in this [link](https://github.com/TomLin/RLND-project/blob/master/p2-continuous-control/Continuous_Control.ipynb). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Result - Low Efficiency\n",
    "\n",
    "Based on prior failure, this time, the training process is a bit different. The training process will start for every **150 timesteps** and again weight-update iterates for **10 times** for each training. \n",
    "\n",
    "I hope this will further stablize the training although it may take much longer time. The following result shows that D4PG agent successfully reaches the target fo average score 30, althought it takes up around **5000 episodes**. The learning progress is really slow.\n",
    "\n",
    "The rolling average scores over episodes are also included in the chart below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Picture of training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chart of rolling scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gif of final training agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on Multi Agent Scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief Background of The Environment\n",
    "\n",
    "This time, I use environment which will activate 20 agents simultaneously, each with its own copy of environment. The experiences of these 20 agents will be gathered up and shared to other agents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_path = '/Users/tomlin/Documents/Github-Repository/RLND/RLND-dataset/p2-continuous-control/Reacher_multiple.app'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "\n",
    "# Import the environment.\n",
    "env = UnityEnvironment(file_name=env_path)\n",
    "\n",
    "# Get default brain name.\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# Reset the environment -> switch to training(episodical) mode,\n",
    "# and extract environment information out by indexing on brain_name.\n",
    "env_info = env.reset(train_mode=True)[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "printmd('**Number of agents:** {}'.format(num_agents))\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "printmd('\\n**Size of each action:** {}'.format(action_size))\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "printmd('\\n**There are** {} **agents. Each observes a state with length:** {}'.format(states.shape[0], state_size))\n",
    "printmd('\\n**The state for the agent looks like:**\\n')\n",
    "display(states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A2C\n",
    "\n",
    "The modules/functions used to build up A2C model are as follows,\n",
    "\n",
    "1. `A2CModel`: Neural network for A2C reinforcement learning algorithm.\n",
    "2. `collect_trajectories`: collect n-step experience transitions.\n",
    "3. `learn`: compute training loss from collected trajectories and update network's weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Structure\n",
    "\n",
    "The model is a simple two fully-connected layers with **128** units, **64** units for each layer. Then it separates out to **actor** and **critic** layer (instead of the actor and critic network in previous models). Both actor and critic layer use fully-connected layer as ways implemented in the original [A3C algorithm paper](https://arxiv.org/abs/1602.01783)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excerpt of A2C model.\n",
    "\n",
    "class A2CModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(A2CModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(N_INPUTS, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.actor = nn.Linear(64, N_ACTIONS) # actor is directly defined as a layer instead of network\n",
    "        self.critic = nn.Linear(64, 1) # critic is directly defined as a layer instead of network\n",
    "        self.std = torch.ones(N_ACTIONS).to(device)\n",
    "        self.dist = torch.distributions.Normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On-Site Transition Trajectory\n",
    "\n",
    "For A2C is an **on-policy** RL algorithm, there is not such thing as replay memory. Instead, it uses the current collected transition experiences to update its network. \n",
    "\n",
    "In the following code chunk, I define `collect_trajectories()` function. It takes in inputs of **A2C model, whole environment, and number of timestamp to collect**. While the model interacts with the environment, all feedbacks are stored in objects such as `batch_s, batch_a, batch_r`. Then when it reaches the required number of timestamp or the episode ends, the function conducts reward normalization and discount on each timestamp and come up with final **target value/processed rewards** for each timestamp - `batch_v_t`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_trajectories(model, env, brain_name, init_states, episode_end, n_steps):\n",
    "    '''\n",
    "    Params\n",
    "    ======\n",
    "        model (object): A2C model\n",
    "        env (object): environment\n",
    "        brain_name (string): brain name of environment\n",
    "        init_states (n_process, state_size) (numpy): initial states for loop\n",
    "        episode_end (bool): tracker of episode end, default False\n",
    "        n_steps (int): number of steps for reward collection\n",
    "    Returns\n",
    "    =======\n",
    "        batch_s (T, n_process, state_size) (numpy): batch of states\n",
    "        batch_a (T, n_process, action_size) (numpy): batch of actions\n",
    "        batch_v_t (T, n_process) (numpy): batch of n-step rewards (aks target value)\n",
    "        accu_rewards (n_process,) (numpy): accumulated rewards for process (being summed up on all process)\n",
    "        init_states (n_process, state_size) (numpy): initial states for next batch\n",
    "        episode_end (bool): tracker of episode end\n",
    "    '''\n",
    "\n",
    "    batch_s = []\n",
    "    batch_a = []\n",
    "    batch_r = []\n",
    "\n",
    "    states = init_states\n",
    "    accu_rewards = np.zeros(init_states.shape[0])\n",
    "\n",
    "    t = 0\n",
    "    while True:\n",
    "        t += 1\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            states = torch.from_numpy(states).float().to(device)\n",
    "            actions_tanh, actions = model.get_action(states)\n",
    "        model.train()\n",
    "        # actions_tanh (n_process, action_size) (tensor), actions limited within (-1,1)\n",
    "        # actions (n_process, action_size) (tensor)\n",
    "        \n",
    "        env_info = env.step(actions_tanh.cpu().data.numpy())[brain_name]\n",
    "        next_states = env_info.vector_observations\n",
    "        rewards = env_info.rewards\n",
    "        dones = env_info.local_done\n",
    "        # next_states (numpy array)\n",
    "        # rewards (list)\n",
    "        # dones (list)\n",
    "        rewards = np.array(rewards)\n",
    "        dones = np.array(dones)\n",
    "        \n",
    "        accu_rewards += rewards\n",
    "\n",
    "        batch_s.append(states.cpu().data.numpy()) # final shape of batch_s (T, n_process, state_size) (list of numpy)\n",
    "        batch_a.append(actions.cpu().data.numpy()) # final shape of batch_a (T, n_process, action_size) (list of numpy)\n",
    "        batch_r.append(rewards) # final shape of batch_r (T, n_process) (list of numpy array)\n",
    "\n",
    "        if dones.any() or t >= n_steps:\n",
    "            model.eval()\n",
    "            next_states = torch.from_numpy(next_states).float().to(device)\n",
    "            final_r = model.get_state_value(next_states).detach().cpu().data.numpy() # final_r (n_process,) (numpy)\n",
    "            model.train()\n",
    "\n",
    "            for i in range(len(dones)):\n",
    "                if dones[i] == True:\n",
    "                    final_r[i] = 0\n",
    "                else:\n",
    "                    final_r[i] = final_r[i]\n",
    "\n",
    "            batch_v_t = [] # compute n-step rewards (aks target value)\n",
    "            batch_r = np.array(batch_r)\n",
    "            \n",
    "            for r in batch_r[::-1]:\n",
    "                mean = np.mean(r)\n",
    "                std = np.std(r)\n",
    "                r = (r - mean)/(std+0.0001) # normalize rewards in n_process on each timestep\n",
    "                final_r = r + GAMMA * final_r # reward discount\n",
    "                batch_v_t.append(final_r)\n",
    "            batch_v_t = np.array(batch_v_t)[::-1] # final shape (T, n_process) (numpy)\n",
    "\n",
    "            break\n",
    "\n",
    "        states = next_states\n",
    "\n",
    "    if dones.any():\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        init_states = env_info.vector_observations\n",
    "        episode_end = True\n",
    "        \n",
    "    else:\n",
    "        init_states = next_states.cpu().data.numpy() # if not done, continue batch collection from last states\n",
    "\n",
    "    batch_s = np.stack(batch_s)\n",
    "    batch_a = np.stack(batch_a)\n",
    "\n",
    "    return batch_s, batch_a, batch_v_t, np.sum(accu_rewards), init_states, episode_end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actions are **sampled from normal distribution** with $\\mu$ and $\\sigma$ are dependent on each different states. Furthermore, the action output is passed through `tanh` activation so that its action values squashed between -1 and 1, as required in this environemnt.\n",
    "\n",
    "Besides, in order to retreive log probability of actons later, there is a tirck I use. I define function `get_action()` to return both actions_tanh and raw actions values. The raw action values are stored in the batch. Then in learning phase, it will be passed to `dist_.log_prob(a)` to get the corresponding log probability for the tanhed actions. \n",
    "\n",
    "--- \n",
    "\n",
    "Critic state value is just the output of state being passed to critic layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excerpt of A2C model.\n",
    "\n",
    "class A2CModel(nn.Module):\n",
    "    \n",
    "    def get_action(self, s):\n",
    "    '''\n",
    "    Params\n",
    "    ======\n",
    "        s (n_process, state_size) (tensor): states\n",
    "    Returns\n",
    "    ======\n",
    "        action_tanh (n_process, action_size) (tensor): action limited within (-1,1)\n",
    "        action (n_process, action_size) (tensor): raw action\n",
    "    '''\n",
    "    s = self.forward(s)\n",
    "    mu = self.actor(s)\n",
    "    dist_ = self.dist(mu, self.std)\n",
    "    action = dist_.sample()\n",
    "    action_tanh = F.tanh(action)\n",
    "    \n",
    "    return action_tanh, action\n",
    "\n",
    "\n",
    "    def get_action_prob(self, s, a):\n",
    "    '''\n",
    "    Params\n",
    "    ======\n",
    "        s (n_process, state_size) (tensor): states\n",
    "        a (n_process, action_size) (tensor): actions\n",
    "\n",
    "    Returns\n",
    "    =======\n",
    "        mu (n_process, action_size) (tensor): mean value of action distribution\n",
    "        self.std (action_size,) (tensor): the standard deviation of every action\n",
    "        log_prob (n_process,) (tensor): log probability of input action\n",
    "    '''\n",
    "\n",
    "    s = self.forward(s)\n",
    "    mu = self.actor(s)\n",
    "    dist_ = self.dist(mu, self.std)\n",
    "    log_prob  = dist_.log_prob(a) # get log probability for the action, for use in training\n",
    "    log_prob = torch.sum(log_prob, dim=1, keepdim=False)\n",
    "    \n",
    "    return mu, self.std, log_prob\n",
    "\n",
    "\n",
    "    def get_state_value(self, s):\n",
    "    '''\n",
    "    \n",
    "    State value is the ouput of critic layer.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        s (n_process, state_size) (tensor): states\n",
    "    Returns\n",
    "    =======\n",
    "        value (n_process,) (tensor)\n",
    "    '''\n",
    "    s = self.forward(s)\n",
    "    value = self.critic(s).squeeze(1)\n",
    "\n",
    "    return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "\n",
    "The loss function in A2C is also known as objective function. \n",
    "\n",
    "`Entropy` is used to encourage action exploration in many algorithms, including A2C model. However, I don't include `entropy` in policy loss as people do in most implementations. The reason is that I am now dealing with a multi-dimensional action space. I have no clue on how to specify the entropy for mulit-dimensional action sapce.\n",
    "\n",
    "Instead, I take reference from [ShangtongZhang's work](https://github.com/ShangtongZhang/DeepRL/blob/master/deep_rl/network/network_heads.py), in which he assumes the $\\sigma$, the variance to encourage action exploration, to be constant so that entropy will be constant in all cases as well. This way, I can drop off entropy from the policy loss.\n",
    "\n",
    "In addition, the value loss function is also sub-part of the policy loss. That leads to my policy loss and value loss as:\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "L_{policy} & = -\\{\\frac{1}{M}\\frac{1}{T}\\sum_{i=1}^M\\sum_{t=1}^T[log(\\pi(s_t))*(R - V(s_t))]\\} \\quad exclude \\; \\beta*H(\\pi) \\\\\n",
    "L_{value} & = \\frac{1}{M}\\frac{1}{T}\\sum_{i=1}^M\\sum_{t=1}^T(R - V(s_t))^2\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(batch_s, batch_a, batch_v_t, model, optimizer):\n",
    "    '''\n",
    "    Params\n",
    "    ======\n",
    "        batch_s (T, n_process, state_size) (numpy)\n",
    "        batch_a (T, n_process, action_size) (numpy): batch of actions\n",
    "        batch_v_t (T, n_process) (numpy): batch of n-step rewards (aks target value)\n",
    "        model (object): A2C model\n",
    "        optimizer (object): model parameter optimizer\n",
    "    Returns\n",
    "    ======\n",
    "        total_loss (int): mean actor-critic loss for each batch \n",
    "    '''\n",
    "\n",
    "    batch_s_ = torch.from_numpy(batch_s).float().to(device)\n",
    "    batch_s_ = batch_s_.view(-1, batch_s.shape[-1]) # shape from (T,n_process,state_size) -> (TxN, state_size)\n",
    "\n",
    "    batch_a_ = torch.from_numpy(batch_a).float().to(device)\n",
    "    batch_a_ = batch_a_.view(-1, batch_a.shape[-1]) # shape from (T,n_process,action_size) -> (TxN, action_size)\n",
    "\n",
    "    values = model.get_state_value(batch_s_) # shape (TxN,)\n",
    "    values = values.view(*batch_s.shape[:2]) # shape (T,n)\n",
    "\n",
    "    # pytorch's problem of negative stride -> require .copy() to create new numpy\n",
    "    batch_v_t_ = torch.from_numpy(batch_v_t.copy()).float().to(device)\n",
    "    td = batch_v_t_ - values # shape (T, n_process) (tensor) ------> Loss of critic value (R - V_st)\n",
    "    c_loss = td.pow(2).mean()\n",
    "\n",
    "    mus, stds, log_probs = model.get_action_prob(batch_s_, batch_a_)\n",
    "    log_probs_ = log_probs.view(*batch_a.shape[:2]) # shape from (TxN,) -> (T,n) (tensor)\n",
    "\n",
    "    a_loss = -((log_probs_ * td.detach()).mean()) # Loss of policy (log_prob * (R - V_st))\n",
    "    total_loss = c_loss + a_loss\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # stds is constnat -> no gradient, no detach()\n",
    "    return total_loss.detach().cpu().data.numpy(), mus.detach().cpu().data.numpy(), stds.cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight Update\n",
    "\n",
    "In A2C model, all weights are directly updated by the gradients of the current trajectory batch, meaning no soft-update applied here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameters in a Nutshell\n",
    "\n",
    "Followings are the overview of hyper-parameter setting,\n",
    "\n",
    "- Number of learning episode: 1000\n",
    "- Number of N-Step: 10\n",
    "- Learning rate: 0.00015\n",
    "- GAMMA: 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Training Process\n",
    "\n",
    "Here, instead of defining a training function as above, I just directly set up the training process to monitor the training progress and save the model after training is finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up training process.\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "agent_a2c = A2CModel().to(device)\n",
    "optimizer = optim.Adam(agent_a2c.parameters(), lr=0.00015)\n",
    "\n",
    "env_info = env.reset(train_mode=True)[brain_name] \n",
    "states = env_info.vector_observations\n",
    "init_states = states\n",
    "\n",
    "n_episodes = 1\n",
    "n_steps = 10\n",
    "episode_end = False\n",
    "a2c_ep_rewards_list = []\n",
    "ep_rewards_deque = deque([0], maxlen=100) # initialize with 0\n",
    "ep_rewards = 0\n",
    "\n",
    "while True:\n",
    "    batch_s, batch_a, batch_v_t, accu_rewards, init_states, episode_end = collect_trajectories(\n",
    "        agent_a2c, env, brain_name, init_states, episode_end, n_steps)\n",
    "\n",
    "    loss, mus, stds = learn(batch_s, batch_a, batch_v_t, agent_a2c, optimizer)\n",
    "    ep_rewards += accu_rewards\n",
    "    print('\\rEpisode {:>4}\\tEpisodic Score {:>7.3f}\\tLoss {:>12.6f}'.format(\n",
    "        n_episodes, np.mean(ep_rewards_deque), float(loss)), end=\"\")\n",
    "\n",
    "    if episode_end == True:\n",
    "        if n_episodes % 100 == 0:\n",
    "            print('\\rEpisode {:>4}\\tEpisodic Score {:>7.3f}\\tLoss {:>12.6f}'.format(\n",
    "                n_episodes, np.mean(ep_rewards_deque), float(loss)))\n",
    "\n",
    "        if np.mean(ep_rewards_deque) >= 34:\n",
    "            break\n",
    "        a2c_ep_rewards_list.append(ep_rewards/num_agents)\n",
    "        ep_rewards_deque.append(ep_rewards/num_agents)\n",
    "        ep_rewards = 0\n",
    "        n_episodes += 1\n",
    "        episode_end = False\n",
    "\n",
    "\n",
    "# save a2c model\n",
    "pth = './checkpoint/a2c_checkpoint.pth'\n",
    "torch.save(agent_a2c.state_dict(), pth)\n",
    "\n",
    "a2c_ep_rewards_list = np.array(a2c_ep_rewards_list)\n",
    "np.save('./data/a2c_ep_rewards_list.npy', a2c_ep_rewards_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Result - High Efficiency\n",
    "\n",
    "In training, once agents collect one new batch of N-Step transition experience, the batch will be used to compute the loss and update the actor and critic's network parameters. Notice the last state of batch will be the initial state of next batch if any of the agents' epsiode is not done yet. Once any of the angents' episode is done, then the episode training will move on to next episode for all agents.\n",
    "\n",
    "From the result below, you can tell A2C is very efficient. The agent learns to pick up the task and reach goal score 30 in **less than 1000 episodes**. Plus, the training experience is quite consistent and stable, you can get pretty the same result whenever you re-train the agent again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Picture of training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chart of rolling scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparision of All Models\n",
    "\n",
    "From these trials, the A2C model has the best performance and efficiency, at the same time the re-train result is very consistent, but given the fact it is a multi-agent training and uses 10 steps trajectory, it shouldn't be too surprising for the outcome. The D4PG in this case is a single agent training, and I only use 1 step trajectory, but it still gives kind of a satisfactory outcome, but the re-train result is not so consistent, you may find your agent stuck in some local optimum in some trials, and the learning is not so efficient either. In my case, it takes 5000 episodes to reach the goal score, but my setting for parameter update is every 150 timestamps, probably I can increase the update frequency to improve its efficiency. Nonetheless, this way I would take the risk that the learning progress is not so stable again. It's basically a trade-off. The last is DDPG, well, I guess I won't use that algorithm for continuous action task ever again. It doesn't work out in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Improvement\n",
    "\n",
    "Possible attempts may include to re-write the code using Python's multiprocessing module, that will enable the algorithm to learn in parallel environment. This is the ongoing trend in reinforcement learning. And I might try to see if I can re-code D4PG to take multiple steps trajectory for learning in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 說明為什麼DDPG只有測試1000個episode，因為之前已經測試過失敗了\n",
    "# 下截已經train好的模型的圖片，並且截圖分享在medium上頭"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
