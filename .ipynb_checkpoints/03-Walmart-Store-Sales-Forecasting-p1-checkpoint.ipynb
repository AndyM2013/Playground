{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from IPython.display import display, Markdown\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "import warnings \n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "np.set_printoptions(suppress=True) # supress scientific notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_parser(x):\n",
    "    return datetime.strptime(x, '%Y-%m-%d')\n",
    "\n",
    "data = pd.read_csv('../Playground-dataset/03-Walmart-Store-Sales-Forecasting-Dataset/train.csv',\n",
    "            header=0,\n",
    "            date_parser=date_parser,\n",
    "            parse_dates=['Date'])\n",
    "\n",
    "data.columns = ['store','dept','date','weekly_sales','is_holiday']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printmd(string):\n",
    "    return display(Markdown(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "printmd('**Preivew of the Raw Dataset.**')\n",
    "display(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join store and dept to single column -> unique identifier \n",
    "data['store_dept'] = data[['store','dept']].apply(\n",
    "    lambda x: '_'.join(x.map(str)), axis=1) # type convert to string first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original data\n",
    "pd.to_pickle(data, '../Playground-dataset/03-Walmart-Store-Sales-Forecasting-Dataset/data.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('../Playground-dataset/03-Walmart-Store-Sales-Forecasting-Dataset/data.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sales Aggregated on Store\n",
    "\n",
    "The total sales of each store is different. Some stores have constantly higher sales while others have lower sales. Probaly it relates to the store size. Nonetheless, if observing the trend of sales, all stores seem to follow quite consistant/similar pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Preprocess data: long to wide on 'store'.\n",
    "wide_store = data.groupby(['store','date'])['weekly_sales'].sum().unstack(['store'])\n",
    "\n",
    "printmd('**Preview of Wide Store:**')\n",
    "display(wide_store.head(n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_pickle(wide_store, \n",
    "             '../Playground-dataset/03-Walmart-Store-Sales-Forecasting-Dataset/wide_store.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_store = pd.read_pickle(\n",
    "    '../Playground-dataset/03-Walmart-Store-Sales-Forecasting-Dataset/wide_store.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TimeSeriesUtils import plot_yaxis_with_break\n",
    "plot_yaxis_with_break(wide_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sales Aggregated on Dept\n",
    "\n",
    "Contrary to what we observe in store sales. The sales pattern on each department seems quite different, thus warranting to separate them and investigate individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Preprocess data: long to wide on 'dept'. \n",
    "wide_dept = data.groupby(['dept','date'])['weekly_sales'].sum().unstack(['dept'])\n",
    "\n",
    "printmd('**Preview of Wide Dept:**')\n",
    "display(wide_dept.head(n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_pickle(wide_dept, \n",
    "             '../Playground-dataset/03-Walmart-Store-Sales-Forecasting-Dataset/wide_dept.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_dept = pd.read_pickle('../Playground-dataset/03-Walmart-Store-Sales-Forecasting-Dataset/wide_dept.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TimeSeriesUtils import plot_yaxis_with_break\n",
    "plot_yaxis_with_break(wide_dept, \n",
    "                      lower_ylim=(0, 4000000), \n",
    "                      upper_ylim=(9000000, 11000000),\n",
    "                      title='Weekly Sales over Dept')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change Rate of Sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Validation on Store\n",
    "\n",
    "From the test below, we are sure there is no inconsecutive week data. No missing sales record for any week on store data. Only the first row is missing, which is understandable for data shifting in this case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if these exists inconsecutive week on store.\n",
    "week_date = pd.Series(wide_store.index.values)\n",
    "week_date_nex = week_date.shift(1)\n",
    "\n",
    "seven_days = timedelta(days=7) # create timedelta object\n",
    "week_date_test = week_date - week_date_nex\n",
    "\n",
    "printmd('**Row Index for Time Interval not Equal to 7 Days:**')\n",
    "week_date_test[week_date_test != seven_days].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Validation on Dept\n",
    "\n",
    "From the test below, we are sure ther is no inconsecutive week data. No missing sales record for any week on department data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if these exists inconsecutive week on dept.\n",
    "week_date = pd.Series(wide_dept.index.values)\n",
    "week_date_nex = week_date.shift(1)\n",
    "\n",
    "seven_days = timedelta(days=7) # create timedelta object\n",
    "week_date_test = week_date - week_date_nex\n",
    "\n",
    "printmd('**Row Index for Time Interval not Equal to 7 Days:**')\n",
    "week_date_test[week_date_test != seven_days].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change Rate of Sales on Store\n",
    "\n",
    "The change rate of sales on stores is quite similar among all stores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create dataset of change rate on store each week.\n",
    "wide_store_shift = wide_store.shift(1)\n",
    "wide_store_rate = (wide_store.fillna(0) - wide_store_shift.fillna(0))/(wide_store_shift.fillna(0)+1)\n",
    "wide_store_rate = wide_store_rate.iloc[1:,:] # drop off first row\n",
    "printmd('**Preview of Change Rate of Sales on Store:**')\n",
    "display(wide_store_rate.head(n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_pickle(wide_store_rate, \n",
    "    '../Playground-dataset/03-Walmart-Store-Sales-Forecasting-Dataset/wide_store_rate.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_store_rate = pd.read_pickle(\n",
    "    '../Playground-dataset/03-Walmart-Store-Sales-Forecasting-Dataset/wide_store_rate.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from TimeSeriesUtils import plot_zoom\n",
    "plot_zoom(wide_store_rate, zoom=(-0.3,0.3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change Rate of Sales on Dept\n",
    "\n",
    "From the change rate of sales on departments, we can tell there are some departments having different trend and pattern to others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset of change rate on dept each week.\n",
    "wide_dept_shift = wide_dept.shift(1)\n",
    "wide_dept_rate = (wide_dept.fillna(0) - wide_dept_shift.fillna(0))/(wide_dept_shift.fillna(0)+1)\n",
    "wide_dept_rate = wide_dept_rate.iloc[1:,:] # drop off first row\n",
    "printmd('**Preview of Change Rate of Sales on Dept:**')\n",
    "display(wide_dept_rate.head(n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_pickle(wide_dept_rate, \n",
    "    '../Playground-dataset/03-Walmart-Store-Sales-Forecasting-Dataset/wide_dept_rate.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_dept_rate = pd.read_pickle(\n",
    "    '../Playground-dataset/03-Walmart-Store-Sales-Forecasting-Dataset/wide_dept_rate.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from TimeSeriesUtils import plot_zoom\n",
    "plot_zoom(wide_dept_rate, zoom=(-5,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Train and Test Dataset\n",
    "\n",
    "Here I will use date `2011-12-31` as splitting point for training and testing dataset. The implementation of time series clustering is based on training dataset only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train and test dataset on store.\n",
    "store_rate_train = wide_store_rate[wide_store_rate.index <= '2011-12-31']\n",
    "store_rate_test = wide_store_rate[wide_store_rate.index > '2011-12-31']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_pickle(store_rate_train, \n",
    "        '../Playground-dataset/03-Walmart-Store-Sales-Forecasting-Dataset/store_rate_train.pickle')\n",
    "\n",
    "pd.to_pickle(store_rate_test, \n",
    "        '../Playground-dataset/03-Walmart-Store-Sales-Forecasting-Dataset/store_rate_test.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_rate_train = pd.read_pickle(\n",
    "    '../Playground-dataset/03-Walmart-Store-Sales-Forecasting-Dataset/store_rate_train.pickle')\n",
    "store_rate_test = pd.read_pickle(\n",
    "    '../Playground-dataset/03-Walmart-Store-Sales-Forecasting-Dataset/store_rate_test.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train and test dataset on dept.\n",
    "dept_rate_train = wide_dept_rate[wide_dept_rate.index <= '2011-12-31']\n",
    "dept_rate_test = wide_dept_rate[wide_dept_rate.index > '2011-12-31']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_pickle(dept_rate_train, \n",
    "        '../Playground-dataset/03-Walmart-Store-Sales-Forecasting-Dataset/dept_rate_train.pickle')\n",
    "\n",
    "pd.to_pickle(dept_rate_test, \n",
    "        '../Playground-dataset/03-Walmart-Store-Sales-Forecasting-Dataset/dept_rate_test.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dept_rate_train = pd.read_pickle(\n",
    "    '../Playground-dataset/03-Walmart-Store-Sales-Forecasting-Dataset/dept_rate_train.pickle')\n",
    "detp_rate_test = pd.read_pickle(\n",
    "    '../Playground-dataset/03-Walmart-Store-Sales-Forecasting-Dataset/dept_rate_test.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering on Time Series - DTW Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Dynamic Time Warping distance -> the smaller the value, the more similar the two time series are.\n",
    "\n",
    "def array_scaler(x):\n",
    "    \"\"\"Scales array to 0-1\n",
    "    \n",
    "    Dependencies:\n",
    "        import numpy as np\n",
    "    Args:\n",
    "        x: mutable iterable array of float\n",
    "    returns:\n",
    "        scaled x\n",
    "    \"\"\"\n",
    "    arr_min = min(x)\n",
    "    x = np.array(x) - float(arr_min)\n",
    "    arr_max = max(x)\n",
    "    x = x/float(arr_max)\n",
    "    return x\n",
    "\n",
    "\n",
    "def dtw_distance(x, y, d=lambda x,y: abs(x-y), scaled=False, fill=False):\n",
    "    \"\"\"Finds the distance of two arrays by dynamic time warping method\n",
    "    source: https://en.wikipedia.org/wiki/Dynamic_time_warping\n",
    "    \n",
    "    Dependencies:\n",
    "        import numpy as np\n",
    "    Args:\n",
    "        x, y: arrays\n",
    "        d: distance function, default is absolute difference\n",
    "        scaled: boolean, should arrays be scaled before calculation\n",
    "        fill: boolean, should NA values be filled with 0\n",
    "    returns:\n",
    "        distance as float, 0.0 means series are exactly same, upper limit is infinite\n",
    "    \"\"\"\n",
    "    if fill:\n",
    "        x = np.nan_to_num(x)\n",
    "        y = np.nan_to_num(y)\n",
    "    if scaled:\n",
    "        x = array_scaler(x)\n",
    "        y = array_scaler(y)\n",
    "    n = len(x) + 1\n",
    "    m = len(y) + 1\n",
    "    DTW = np.zeros((n, m))\n",
    "    DTW[:, 0] = float('Inf')\n",
    "    DTW[0, :] = float('Inf')\n",
    "    DTW[0, 0] = 0\n",
    "    \n",
    "    for i in range(1, n):\n",
    "        for j in range(1, m):\n",
    "            cost = d(x[i-1], y[j-1])\n",
    "            DTW[i, j] = cost + min(DTW[i-1, j], DTW[i, j-1], DTW[i-1, j-1])\n",
    "\n",
    "    return DTW[n-1, m-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On Store Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_rt_list = store_rate_train.to_dict('list')\n",
    "store_keys = pd.Series(list(store_rt_list.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtw_matrix = pd.DataFrame(\n",
    "    store_keys.apply(lambda x: store_keys.apply(\n",
    "        lambda y: dtw_distance(store_rt_list[x], store_rt_list[y], scaled=False, fill=True))))\n",
    "\n",
    "dtw_matrix.columns, dtw_matrix.index = store_keys, store_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hierarchical clustering on time series.\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.spatial.distance import squareform\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists_ = squareform(dtw_matrix)\n",
    "dists_\n",
    "\n",
    "# The first argument of linkage should not be the square distance matrix. \n",
    "# It must be the condensed distance matrix.\n",
    "linkage_matrix = linkage(dists_, \"complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "dendrogram(linkage_matrix)\n",
    "plt.title(\"Time Series Clustering\")\n",
    "plt.tick_params(labelsize='xx-large')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use bulit-in sklearn hierarchical clustering to make cluster perdiction.\n",
    "agg_model = AgglomerativeClustering(n_clusters=5, affinity='precomputed', linkage='complete')\n",
    "labels = pd.Series(agg_model.fit_predict(dtw_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the cluster label as dataframe.\n",
    "store_labels = pd.DataFrame(\n",
    "    {'store_label':labels,\n",
    "     'store':dtw_matrix.index.values})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_pickle(store_labels, \n",
    "    '../Playground-dataset/03-Walmart-Store-Sales-Forecasting-Dataset/store_labels.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_labels = pd.read_pickle(\n",
    "    '../Playground-dataset/03-Walmart-Store-Sales-Forecasting-Dataset/store_labels.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On Department Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dept_rt_list = dept_rate_train.to_dict('list')\n",
    "dept_keys = pd.Series(list(dept_rt_list.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtw_matrix = pd.DataFrame(\n",
    "    dept_keys.apply(lambda x: dept_keys.apply(\n",
    "        lambda y: dtw_distance(dept_rt_list[x], dept_rt_list[y], scaled=False, fill=True))))\n",
    "\n",
    "dtw_matrix.columns, dtw_matrix.index = dept_keys, dept_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find out the abnormal department.\n",
    "# printmd('**Department that is so Dissimilar to other Departments:**')\n",
    "# display(np.argmax(dtw_matrix.iloc[0,:], axis=1))\n",
    "\n",
    "# Remove that department.\n",
    "# dtw_matrix_sub = dtw_matrix.loc[dtw_matrix.index != 77,:]\n",
    "# dtw_matrix_sub = dtw_matrix_sub.loc[:, dtw_matrix_sub.columns != 77]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hierarchical clustering on time series.\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.spatial.distance import squareform\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists_ = squareform(dtw_matrix)\n",
    "dists_\n",
    "\n",
    "# The first argument of linkage should not be the square distance matrix. \n",
    "# It must be the condensed distance matrix.\n",
    "linkage_matrix = linkage(dists_, \"complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dendrogram(linkage_matrix)\n",
    "plt.title(\"Time Series Clustering\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Outliers\n",
    "\n",
    "From the dendrogram above, we can't clearly identify the patterns of other major department sells owing to some outliers. Here, I'd first exclude these outliers and try to find patterns in major time series. Then, I'd again try to cluster outlier only series, so that we can better observe the patterns on both normal and outlier time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dept_rate_train.T\n",
    "printmd('**Preview of df:**')\n",
    "display(df.head(n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z-score method\n",
    "from scipy import stats\n",
    "\n",
    "def identify_outlier(df, zthreshold=3):\n",
    "    \"\"\"\n",
    "    @param\n",
    "        df: row are each time series, columns are the datetimes\n",
    "    \"\"\"\n",
    "    final_set = set()\n",
    "\n",
    "    for col in df.columns:\n",
    "        zscores = stats.zscore(df.loc[:,col])\n",
    "        row_num = np.where(zscores > zthreshold)[0] # row number\n",
    "        idxes = list(df.index.values[row_num])\n",
    "        idxes = set(idxes)\n",
    "        final_set = final_set.union(idxes)\n",
    "    \n",
    "    final_set = sorted(list(final_set))\n",
    "\n",
    "    print('Outliers Being Detected: {}'.format(final_set))\n",
    "    print('Number of Outliers: {}'.format(len(final_set)))\n",
    "    print('Number of Total Samples: {}'.format(len(df.index)))\n",
    "    \n",
    "    return final_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get indexes of outlier rows.\n",
    "final_set = identify_outlier(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate good data points and outlier data points.\n",
    "good_df = df.drop(final_set)\n",
    "good_dept_df = good_df.T\n",
    "outlier_df = df.loc[final_set,:]\n",
    "outlier_dept_df = outlier_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_pickle(good_dept_df,\n",
    "    '../Playground-dataset/03-Walmart-Store-Sales-Forecasting-Dataset/good_dept_df.pickle')\n",
    "\n",
    "pd.to_pickle(outlier_dept_df,\n",
    "    '../Playground-dataset/03-Walmart-Store-Sales-Forecasting-Dataset/outlier_dept_df.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_dept_df = pd.read_pickle(\n",
    "    '../Playground-dataset/03-Walmart-Store-Sales-Forecasting-Dataset/good_dept_df.pickle')\n",
    "outlier_dept_df = pd.read_pickle(\n",
    "    '../Playground-dataset/03-Walmart-Store-Sales-Forecasting-Dataset/outlier_dept_df.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On Dept - Outlier Excluded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dept_rt_list = good_dept_df.to_dict('list')\n",
    "dept_keys = pd.Series(list(dept_rt_list.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtw_matrix = pd.DataFrame(\n",
    "    dept_keys.apply(lambda x: dept_keys.apply(\n",
    "        lambda y: dtw_distance(dept_rt_list[x], dept_rt_list[y], scaled=False, fill=True))))\n",
    "\n",
    "dtw_matrix.columns, dtw_matrix.index = dept_keys, dept_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hierarchical clustering on time series.\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.spatial.distance import squareform\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists_ = squareform(dtw_matrix)\n",
    "dists_\n",
    "\n",
    "# The first argument of linkage should not be the square distance matrix. \n",
    "# It must be the condensed distance matrix.\n",
    "linkage_matrix = linkage(dists_, \"complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "dendrogram(linkage_matrix)\n",
    "plt.title(\"Time Series Clustering\")\n",
    "plt.tick_params(labelsize='xx-large')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use bulit-in sklearn hierarchical clustering to make cluster perdiction.\n",
    "agg_model = AgglomerativeClustering(n_clusters=4, affinity='precomputed', linkage='complete')\n",
    "labels = pd.Series(agg_model.fit_predict(dtw_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the cluster label as dataframe.\n",
    "normal_dept_labels = pd.DataFrame(\n",
    "    {'dept_label': labels,\n",
    "     'dept':dtw_matrix.index.values})\n",
    "display(normal_dept_labels.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_pickle(normal_dept_labels, \n",
    "    '../Playground-dataset/03-Walmart-Store-Sales-Forecasting-Dataset/normal_dept_labels.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_dept_labels = pd.read_pickle(\n",
    "    '../Playground-dataset/03-Walmart-Store-Sales-Forecasting-Dataset/normal_dept_labels.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  On Dept - Focus on Outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dept_rt_list = outlier_dept_df.to_dict('list')\n",
    "dept_keys = pd.Series(list(dept_rt_list.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtw_matrix = pd.DataFrame(\n",
    "    dept_keys.apply(lambda x: dept_keys.apply(\n",
    "        lambda y: dtw_distance(dept_rt_list[x], dept_rt_list[y], scaled=False, fill=True))))\n",
    "\n",
    "dtw_matrix.columns, dtw_matrix.index = dept_keys, dept_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hierarchical clustering on time series.\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.spatial.distance import squareform\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtw_matrix_sub = dtw_matrix.drop(6) # drop row of index 6\n",
    "# dtw_matrix_sub = dtw_matrix_sub.drop(6,axis=1) # drop column of index 6\n",
    "\n",
    "dists_ = squareform(dtw_matrix)\n",
    "dists_\n",
    "\n",
    "# The first argument of linkage should not be the square distance matrix. \n",
    "# It must be the condensed distance matrix.\n",
    "linkage_matrix = linkage(dists_, \"complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "dendrogram(linkage_matrix)\n",
    "plt.title(\"Time Series Clustering\")\n",
    "plt.tick_params(labelsize='xx-large')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use bulit-in sklearn hierarchical clustering to make cluster perdiction.\n",
    "agg_model = AgglomerativeClustering(n_clusters=3, affinity='precomputed', linkage='complete')\n",
    "labels = pd.Series(agg_model.fit_predict(dtw_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the cluster label as dataframe.\n",
    "outlier_dept_labels = pd.DataFrame(\n",
    "    {'dept_label':labels + 5,\n",
    "     'dept':dtw_matrix.index.values})\n",
    "display(outlier_dept_labels.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_pickle(outlier_dept_labels, \n",
    "    '../Playground-dataset/03-Walmart-Store-Sales-Forecasting-Dataset/outlier_dept_labels.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_dept_labels = pd.read_pickle(\n",
    "    '../Playground-dataset/03-Walmart-Store-Sales-Forecasting-Dataset/outlier_dept_labels.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append dept labels of both normal dataset and outlier dataset.\n",
    "dept_labels = normal_dept_labels.append(outlier_dept_labels).reset_index(drop=True)\n",
    "\n",
    "printmd('**Preview of Dept Labels:**')\n",
    "display(dept_labels.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_pickle(dept_labels, \n",
    "    '../Playground-dataset/03-Walmart-Store-Sales-Forecasting-Dataset/dept_labels.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dept_labels = pd.read_pickle(\n",
    "    '../Playground-dataset/03-Walmart-Store-Sales-Forecasting-Dataset/dept_labels.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization on Clustering Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cluster_result(df, class_df, label_col, seg_col):\n",
    "    num_chart = len(class_df[label_col].unique())\n",
    "    class_values = class_df[label_col].unique()\n",
    "    \n",
    "    sns.set_style('whitegrid')\n",
    "    fig, ax = plt.subplots(num_chart,1, figsize=(20,25), sharex=True)\n",
    "    ax = ax.ravel()\n",
    "    \n",
    "    for i in range(num_chart):\n",
    "        segs = list(class_df[seg_col][class_df[label_col] == class_values[i]])\n",
    "        sub_df = df.loc[:,[v in segs for v in df.columns]]\n",
    "        _ = sub_df.plot(kind='line', legend=False, ax=ax[i], colormap='tab20c')\n",
    "        \n",
    "\n",
    "    _ = plt.tick_params(labelsize='large')\n",
    "    _ = plt.xlabel('date', fontsize='xx-large')\n",
    "    _ = fig.suptitle('{} Growth Rate'.format(seg_col.capitalize()), y=0.9, fontsize='xx-large')\n",
    "    _ = plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_cluster_result(store_rate_train, class_df=store_labels, label_col='store_label', seg_col='store')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cluster_result(dept_rate_train, class_df=dept_labels, label_col='dept_label', seg_col='dept')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change Rate for each (Store, Dept) Pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refer back to the original dataset.\n",
    "data = pd.read_pickle('../Playground-dataset/03-Walmart-Store-Sales-Forecasting-Dataset/data.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store `is_holiday` in a Separate Dataframe\n",
    "\n",
    "The task requires to distinguish sales of holiday from non-holiday. The error on sales prediction is weighted heavier on holiday sales. Thus, before implementing feature engineering on weekly sales, I'd save the is_holiday flag to separate dataframe. Then I will join the holiday flag back when the data is processed and readay for use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store `is_holiday` in a separate df.\n",
    "is_holiday = data[['date','is_holiday']]\n",
    "is_holiday_unq = is_holiday[~is_holiday.duplicated()] # store unique date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Consecutive Time Series for each (Store, Dept) Pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store</th>\n",
       "      <th>dept</th>\n",
       "      <th>date</th>\n",
       "      <th>weekly_sales</th>\n",
       "      <th>store_dept</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-02-05</td>\n",
       "      <td>24924.50</td>\n",
       "      <td>1_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-02-12</td>\n",
       "      <td>46039.49</td>\n",
       "      <td>1_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-02-19</td>\n",
       "      <td>41595.55</td>\n",
       "      <td>1_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-02-26</td>\n",
       "      <td>19403.54</td>\n",
       "      <td>1_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-03-05</td>\n",
       "      <td>21827.90</td>\n",
       "      <td>1_1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   store  dept       date  weekly_sales store_dept\n",
       "0      1     1 2010-02-05      24924.50        1_1\n",
       "1      1     1 2010-02-12      46039.49        1_1\n",
       "2      1     1 2010-02-19      41595.55        1_1\n",
       "3      1     1 2010-02-26      19403.54        1_1\n",
       "4      1     1 2010-03-05      21827.90        1_1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.drop('is_holiday', axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 7 days series -> Time series of consecutive weeks for each (store, dept)\n",
    "from TimeSeriesUtils import consecutive_series\n",
    "days_series = consecutive_series(df=data, col='date', day_width=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the (store, dept) flag to the days_series,\n",
    "# so that we have full consecutive weekday for each (store, dept).\n",
    "from TimeSeriesUtils import cross_join_key_series\n",
    "\n",
    "final_df = cross_join_key_series(df=data, key='store_dept', series=days_series)\n",
    "final_df['store'] = final_df['store_dept'].apply(lambda x: float(x.split('_')[0])) # separate store\n",
    "final_df['dept'] = final_df['store_dept'].apply(lambda x: float(x.split('_')[1])) # separate dept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cross join the original data to the consecutive time series.\n",
    "data_full = pd.merge(\n",
    "    data, final_df, how='right', \n",
    "    left_on=['store','dept','store_dept','date'], \n",
    "    right_on=['store','dept','store_dept','date'])\n",
    "data_full = data_full.sort_values(['store_dept', 'date']).reset_index(drop=True)\n",
    "data_full['weekly_sales'] = data_full['weekly_sales'].fillna(0) # fill in 0 for any missing value\n",
    "data_full.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Validation on each (store, dept) Pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date validation on (store, dept) pair, check for inconsecutive week.\n",
    "data_pre = data_full.groupby(\n",
    "    ['store_dept'])['date'].transform(lambda x: x.shift(1)) # shift one week later\n",
    "data_pre.rename('date_lag', inplace=True)\n",
    "data_pre.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check the rows of NaT (null value for datetime).\n",
    "data_pre[data_pre.isnull()][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate original data and date_lag.\n",
    "data_full = pd.concat([data_full,data_pre], axis=1)\n",
    "\n",
    "# Preview of the result.\n",
    "print('Preview of what the data looks like on rows containing missing values.')\n",
    "display(data_full[425:432])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check if exist rows with timedelta is different 7 days range.\n",
    "data_full['days_lag'] = data_full['date'] - data_full['date_lag']\n",
    "\n",
    "seven_days = timedelta(days=7) # create timedelta object\n",
    "data_full['check_days_lag'] = data_full['days_lag'] != seven_days # boolean for 7 days range\n",
    "\n",
    "\n",
    "# Check for rows with timedelta having different day range, excluding rows of missing values\n",
    "display(data_full[~data_full['date_lag'].isnull() & data_full['check_days_lag']][:5])\n",
    "print('All rows are either missing values or having 7 days range.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_pickle(data_full, \n",
    "             '../Playground-dataset/03-Walmart-Store-Sales-Forecasting-Dataset/data_full.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_full = pd.read_pickle(\n",
    "    '../Playground-dataset/03-Walmart-Store-Sales-Forecasting-Dataset/data_full.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Lag Sales Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         NaN\n",
       "1    40212.84\n",
       "2    67699.32\n",
       "3    49748.33\n",
       "4    33601.22\n",
       "Name: weekly_sales_lag_1, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from TimeSeriesUtils import create_lag_sales\n",
    "\n",
    "# sales lag 1 period\n",
    "col_previous_1 = create_lag_sales(df=data_full, \n",
    "                                groupby_col=['store_dept'], \n",
    "                                shift_col='weekly_sales',\n",
    "                                shift_num=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate original data and sales_lag.\n",
    "data_full = pd.concat([data_full,col_previous_1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         NaN\n",
       "1         NaN\n",
       "2    40212.84\n",
       "3    67699.32\n",
       "4    49748.33\n",
       "Name: weekly_sales_lag_2, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sales lag 2 period.\n",
    "col_previous_2 = create_lag_sales(df=data_full, \n",
    "                                groupby_col=['store_dept'], \n",
    "                                shift_col='weekly_sales',\n",
    "                                shift_num=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate original data and sales_lag.\n",
    "data_full = pd.concat([data_full,col_previous_2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         NaN\n",
       "1         NaN\n",
       "2         NaN\n",
       "3    40212.84\n",
       "4    67699.32\n",
       "Name: weekly_sales_lag_3, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sales lag 3 period.\n",
    "col_previous_3 = create_lag_sales(df=data_full, \n",
    "                                groupby_col=['store_dept'], \n",
    "                                shift_col='weekly_sales',\n",
    "                                shift_num=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate original data and sales_lag.\n",
    "data_full = pd.concat([data_full,col_previous_3], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute for Change Rate\n",
    "\n",
    "Notice that `weekly_sales` contains negative values (don't konw why?). Thus, when computing for change rate, be aware of the unexpected result out of the divion on zero (especially on denominator)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_chg(cola, colb):\n",
    "    \"\"\"\n",
    "    Compute for change rate for weekly sales.\n",
    "    Special handles for missing values, divion by zero etc.\n",
    "    \n",
    "    @param:\n",
    "        cola: series A\n",
    "        colb: series B\n",
    "    \n",
    "    \"\"\"\n",
    "    if pd.isnull(colb):\n",
    "        return np.nan\n",
    "    elif colb == 0:\n",
    "        return cola/1 - 1\n",
    "    else:\n",
    "        return cola/colb - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store</th>\n",
       "      <th>dept</th>\n",
       "      <th>date</th>\n",
       "      <th>weekly_sales</th>\n",
       "      <th>store_dept</th>\n",
       "      <th>date_lag</th>\n",
       "      <th>days_lag</th>\n",
       "      <th>check_days_lag</th>\n",
       "      <th>weekly_sales_lag_1</th>\n",
       "      <th>weekly_sales_lag_2</th>\n",
       "      <th>weekly_sales_lag_3</th>\n",
       "      <th>sales_chg_rt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-02-05</td>\n",
       "      <td>40212.84</td>\n",
       "      <td>10_1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-02-12</td>\n",
       "      <td>67699.32</td>\n",
       "      <td>10_1</td>\n",
       "      <td>2010-02-05</td>\n",
       "      <td>7 days</td>\n",
       "      <td>False</td>\n",
       "      <td>40212.84</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.683525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-02-19</td>\n",
       "      <td>49748.33</td>\n",
       "      <td>10_1</td>\n",
       "      <td>2010-02-12</td>\n",
       "      <td>7 days</td>\n",
       "      <td>False</td>\n",
       "      <td>67699.32</td>\n",
       "      <td>40212.84</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.265158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-02-26</td>\n",
       "      <td>33601.22</td>\n",
       "      <td>10_1</td>\n",
       "      <td>2010-02-19</td>\n",
       "      <td>7 days</td>\n",
       "      <td>False</td>\n",
       "      <td>49748.33</td>\n",
       "      <td>67699.32</td>\n",
       "      <td>40212.84</td>\n",
       "      <td>-0.324576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-03-05</td>\n",
       "      <td>36572.44</td>\n",
       "      <td>10_1</td>\n",
       "      <td>2010-02-26</td>\n",
       "      <td>7 days</td>\n",
       "      <td>False</td>\n",
       "      <td>33601.22</td>\n",
       "      <td>49748.33</td>\n",
       "      <td>67699.32</td>\n",
       "      <td>0.088426</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   store  dept       date  weekly_sales store_dept   date_lag days_lag  \\\n",
       "0     10     1 2010-02-05      40212.84       10_1        NaT      NaT   \n",
       "1     10     1 2010-02-12      67699.32       10_1 2010-02-05   7 days   \n",
       "2     10     1 2010-02-19      49748.33       10_1 2010-02-12   7 days   \n",
       "3     10     1 2010-02-26      33601.22       10_1 2010-02-19   7 days   \n",
       "4     10     1 2010-03-05      36572.44       10_1 2010-02-26   7 days   \n",
       "\n",
       "   check_days_lag  weekly_sales_lag_1  weekly_sales_lag_2  weekly_sales_lag_3  \\\n",
       "0            True                 NaN                 NaN                 NaN   \n",
       "1           False            40212.84                 NaN                 NaN   \n",
       "2           False            67699.32            40212.84                 NaN   \n",
       "3           False            49748.33            67699.32            40212.84   \n",
       "4           False            33601.22            49748.33            67699.32   \n",
       "\n",
       "   sales_chg_rt  \n",
       "0           NaN  \n",
       "1      0.683525  \n",
       "2     -0.265158  \n",
       "3     -0.324576  \n",
       "4      0.088426  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute sales change rate. -> target variable\n",
    "data_full['sales_chg_rt'] = data_full.apply(\n",
    "    lambda x: compute_chg(x['weekly_sales'],x['weekly_sales_lag_1']), axis=1)\n",
    "data_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store</th>\n",
       "      <th>dept</th>\n",
       "      <th>date</th>\n",
       "      <th>weekly_sales</th>\n",
       "      <th>store_dept</th>\n",
       "      <th>date_lag</th>\n",
       "      <th>days_lag</th>\n",
       "      <th>check_days_lag</th>\n",
       "      <th>weekly_sales_lag_1</th>\n",
       "      <th>weekly_sales_lag_2</th>\n",
       "      <th>weekly_sales_lag_3</th>\n",
       "      <th>sales_chg_rt</th>\n",
       "      <th>sales_chg_rt_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-02-05</td>\n",
       "      <td>40212.84</td>\n",
       "      <td>10_1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-02-12</td>\n",
       "      <td>67699.32</td>\n",
       "      <td>10_1</td>\n",
       "      <td>2010-02-05</td>\n",
       "      <td>7 days</td>\n",
       "      <td>False</td>\n",
       "      <td>40212.84</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.683525</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-02-19</td>\n",
       "      <td>49748.33</td>\n",
       "      <td>10_1</td>\n",
       "      <td>2010-02-12</td>\n",
       "      <td>7 days</td>\n",
       "      <td>False</td>\n",
       "      <td>67699.32</td>\n",
       "      <td>40212.84</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.265158</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-02-26</td>\n",
       "      <td>33601.22</td>\n",
       "      <td>10_1</td>\n",
       "      <td>2010-02-19</td>\n",
       "      <td>7 days</td>\n",
       "      <td>False</td>\n",
       "      <td>49748.33</td>\n",
       "      <td>67699.32</td>\n",
       "      <td>40212.84</td>\n",
       "      <td>-0.324576</td>\n",
       "      <td>0.683525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-03-05</td>\n",
       "      <td>36572.44</td>\n",
       "      <td>10_1</td>\n",
       "      <td>2010-02-26</td>\n",
       "      <td>7 days</td>\n",
       "      <td>False</td>\n",
       "      <td>33601.22</td>\n",
       "      <td>49748.33</td>\n",
       "      <td>67699.32</td>\n",
       "      <td>0.088426</td>\n",
       "      <td>-0.265158</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   store  dept       date  weekly_sales store_dept   date_lag days_lag  \\\n",
       "0     10     1 2010-02-05      40212.84       10_1        NaT      NaT   \n",
       "1     10     1 2010-02-12      67699.32       10_1 2010-02-05   7 days   \n",
       "2     10     1 2010-02-19      49748.33       10_1 2010-02-12   7 days   \n",
       "3     10     1 2010-02-26      33601.22       10_1 2010-02-19   7 days   \n",
       "4     10     1 2010-03-05      36572.44       10_1 2010-02-26   7 days   \n",
       "\n",
       "   check_days_lag  weekly_sales_lag_1  weekly_sales_lag_2  weekly_sales_lag_3  \\\n",
       "0            True                 NaN                 NaN                 NaN   \n",
       "1           False            40212.84                 NaN                 NaN   \n",
       "2           False            67699.32            40212.84                 NaN   \n",
       "3           False            49748.33            67699.32            40212.84   \n",
       "4           False            33601.22            49748.33            67699.32   \n",
       "\n",
       "   sales_chg_rt  sales_chg_rt_2  \n",
       "0           NaN             NaN  \n",
       "1      0.683525             NaN  \n",
       "2     -0.265158             NaN  \n",
       "3     -0.324576        0.683525  \n",
       "4      0.088426       -0.265158  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute sales change rate. -> feature\n",
    "data_full['sales_chg_rt_2'] = data_full.apply(\n",
    "    lambda x: compute_chg(x['weekly_sales_lag_2'],x['weekly_sales_lag_3']), axis=1)\n",
    "data_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store</th>\n",
       "      <th>dept</th>\n",
       "      <th>date</th>\n",
       "      <th>weekly_sales</th>\n",
       "      <th>store_dept</th>\n",
       "      <th>date_lag</th>\n",
       "      <th>days_lag</th>\n",
       "      <th>check_days_lag</th>\n",
       "      <th>weekly_sales_lag_1</th>\n",
       "      <th>weekly_sales_lag_2</th>\n",
       "      <th>weekly_sales_lag_3</th>\n",
       "      <th>sales_chg_rt</th>\n",
       "      <th>sales_chg_rt_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [store, dept, date, weekly_sales, store_dept, date_lag, days_lag, check_days_lag, weekly_sales_lag_1, weekly_sales_lag_2, weekly_sales_lag_3, sales_chg_rt, sales_chg_rt_2]\n",
       "Index: []"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for `inf` values.\n",
    "data_full[(data_full['sales_chg_rt'] == -np.inf) | (data_full['sales_chg_rt_2'] == -np.inf)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change Rate of Last Year\n",
    "\n",
    "Let's include the change rate in the same period of last year as one of the predictive features. The same period of last year is assigned on the closest weekday(Friday) relative to the same date(Friday) this year. \n",
    "\n",
    "For instance, if the date last year relative to the same date this year is on Friday, then it's just the same date last year being assigned. If the date in last year is on Saturday, Sunday, Monday, then it's the previous friday that's assigend as the coresponding date in last year. If it's on Tuesday, Wednesday, Thursday, then it's the next friday that's assigned as the corresponding date in last year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the relative week on last year.\n",
    "def date_last_year(date, weekday):\n",
    "    if weekday == 4:\n",
    "        return date\n",
    "    elif weekday == 5:\n",
    "        return (date - timedelta(days=1))\n",
    "    elif weekday == 6:\n",
    "        return (date - timedelta(days=2))\n",
    "    elif weekday == 0:\n",
    "        return (date - timedelta(days=4))\n",
    "    elif weekday == 1:\n",
    "        return (date + timedelta(days=3))\n",
    "    elif weekday == 2:\n",
    "        return (date + timedelta(days=2))\n",
    "    elif weekday == 3:\n",
    "        return (date + timedelta(days=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the corresponding date of last year.\n",
    "data_full['date_last_yr'] = np.nan\n",
    "\n",
    "with tqdm_notebook(total=data_full.shape[0]) as pbar:\n",
    "\n",
    "    for i in range(data_full.shape[0]):\n",
    "        year = data_full['date'][i].year\n",
    "        month = data_full['date'][i].month\n",
    "        day = data_full['date'][i].day\n",
    "\n",
    "        date = str(year-1) + '-' + '{:02d}'.format(month) + '-' + '{:02d}'.format(day) \n",
    "        try:\n",
    "            date = datetime.strptime(date, '%Y-%m-%d') # cope with 29th of Feb.\n",
    "        except:\n",
    "            date = datetime.strptime(str(year-1) + '-02-28', '%Y-%m-%d')\n",
    "\n",
    "        weekday = date.weekday()\n",
    "\n",
    "        date = date_last_year(date, weekday)\n",
    "        data_full['date_last_yr'][i] = date\n",
    "        pbar.update(1)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_pickle(data_full,\n",
    "             '../Playground-dataset/03-Walmart-Store-Sales-Forecasting-Dataset/data_full_2.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_full_2 = pd.read_pickle(\n",
    "    '../Playground-dataset/03-Walmart-Store-Sales-Forecasting-Dataset/data_full_2.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_full_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_full_2['date_last_yr'] = pd.to_datetime(data_full_2['date_last_yr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_full_2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Join the sales change rate of last year.\n",
    "data_full_2 = pd.merge(data_full_2, data_full_2[['date','store_dept','sales_chg_rt']], how='left', \n",
    "    left_on=['date_last_yr', 'store_dept'], right_on=['date','store_dept'])\n",
    "\n",
    "data_full_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign Cluster Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign cluster number for store.\n",
    "data_full_3 = pd.merge(data_full_2, store_labels,\n",
    "                     left_on='store', right_on='store', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign cluster number for dept.\n",
    "data_full_3 = pd.merge(data_full_3, dept_labels,\n",
    "                     left_on='dept', right_on='dept', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Datetime Feature\n",
    "\n",
    "Split month and week of month from `date` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_full_3['date_x_month'] = data_full_3['date_x'].dt.month # month\n",
    "data_full_3['date_x_wom'] = data_full_3['date_x'].apply(lambda x: (x.day-1)//7 + 1) # week of month\n",
    "data_full_3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dummy Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to keep.\n",
    "keep_col = ['store_dept','date_x','sales_chg_rt_x',\n",
    "            'sales_chg_rt_2','sales_chg_rt_y',\n",
    "            'store_label','dept_label',\n",
    "            'date_x_month','date_x_wom']\n",
    "\n",
    "data_full_4 = data_full_3[keep_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_features = ['store_label', 'dept_label','date_x_month','date_x_wom']\n",
    "\n",
    "for c in dummy_features:\n",
    "    dummy = pd.get_dummies(data_full_4[c], prefix=c, drop_first=False)\n",
    "    data_full_4 = pd.concat([data_full_4, dummy], axis=1)\n",
    "\n",
    "drop_col = ['store_label', 'dept_label','date_x_month','date_x_wom']\n",
    "data_full_4 = data_full_4.drop(drop_col, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_full_4.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Training, Validation and Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with missing values.\n",
    "final_data = data_full_4[~data_full_4.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(final_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_pickle(final_data,\n",
    "            '../Playground-dataset/03-Walmart-Store-Sales-Forecasting-Dataset/final_data.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = pd.read_pickle(\n",
    "    '../Playground-dataset/03-Walmart-Store-Sales-Forecasting-Dataset/final_data.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.DataFrame()\n",
    "validation_data = pd.DataFrame()\n",
    "test_data = pd.DataFrame()\n",
    "\n",
    "# Create train and test dataset for each store_dept.\n",
    "for k, group in final_data.groupby('store_dept'):\n",
    "    train_g = group[group['date_x'] <= '2011-12-31'] # match the criterion for clustering time series\n",
    "    remain = group[group['date_x'] > '2011-12-31']\n",
    "    remain_half_row = remain.shape[0] // 2\n",
    "    train_data = train_data.append(train_g)\n",
    "    validation_data = validation_data.append(remain.iloc[:remain_half_row,:])\n",
    "    test_data = test_data.append(remain.iloc[remain_half_row:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data\n",
    "train_y = train_data[['store_dept','date_x','sales_chg_rt_x']]\n",
    "train_feature = train_data.drop(['store_dept','date_x','sales_chg_rt_x'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation data\n",
    "valitation_y = validation_data[['store_dept','date_x','sales_chg_rt_x']]\n",
    "validation_feature = validation_data.drop(['store_dept','date_x','sales_chg_rt_x'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data\n",
    "test_y = test_data[['store_dept','date_x','sales_chg_rt_x']]\n",
    "test_feature = test_data.drop(['store_dept','date_x','sales_chg_rt_x'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "271.528px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
